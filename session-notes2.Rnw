\documentclass[10pt]{article}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{xfrac}
%\usepackage{listings}
%\usepackage[pdftex]{graphicx}
%\DeclareGraphicsExtensions{.png,.pdf}

% attempt to use fontspec to select fonts 
\usepackage{fontspec,xltxtra,xunicode}
\defaultfontfeatures{Mapping=tex-text}
\setsansfont[Scale=MatchLowercase,Mapping=tex-text]{Gill Sans}

\title{Statistical Process Control : Session \# 2 }
\author{Alan Beale}
\date{January 26, 2011}
\begin{document}

\sffamily 
\maketitle

\SweaveOpts{prefix.string=tex/sessionNotes2}

<<setup,echo=F>>=
library("ggplot2")
library("tikzDevice")
options( tikzMetricsDictionary = NULL )
options( tikzLatex = "/usr/texbin/xelatex" )

@ 
\section*{Question 1}
Regarding subgroup size, explain to a person who has taken a statistics course and believes it is important to always obtain large samples why it would be inadvisable to use a subgroup size of at least 30 for an X-bar chart.

\subsection*{web research}
excerpted from http://www.qualitydigest.com/dec98/html/spctool.html

Determine the subgroup size and sampling frequency. Typically, subgroups of three, five or seven units are used, and subgroups of five are most common. Odd-sized subgroups are usually selected because no calculations are necessary when the subgroup size is odd. The subgroup size affects the sensitivity of the control chart. Smaller subgroups create a control chart that is less sensitive to changes in the process, while larger subgroups may be too sensitive to small, economically unimportant changes. When using x charts, the subgroup size should be kept small -- nine or less. Median charts are less efficient statistically than  charts, and the inefficiency increases as the subgroup size increases.

\subsection*{excerpt \#2}
http://www.winspc.com/datanet-quality-systems/support--resources/subgroup-size-x-bar-chart-part-1.html

We see that as the subgroup size increases, the standard deviation of the distribution of averages decreases. Specifically, the relationship below relates the standard deviation of averages to the standard deviation of individuals and the subgroup size: 

On an x-bar control chart, this idea is reflected by control limits getting tighter as the subgroup size increases.

Now, consider a process that is stable and under statistical control. This curve is represented by the blue curve on the graphic below. Suppose we desire that if the process average shifts by a specified amount (such that it is represented by the red curve below), we would want to obtain a chart signal with very high probability. 


\large$ n= \sfrac{(Z_{\sfrac{a}{2}}+Z_{\beta})^2\sigma^2} {D^2} $

\subsection*{answer}
In section 4.15 of the text in mentions that subgroups size is often a matter of convenience and economics. As with any screening technique, the cost of evaluating false positive can significant economic concern. Increasing the subgroup size to 30 for an $\bar{X} $ chart increase initial collection time and time to detect a mean shift.  Using a subgroup size of 30 also increase the change of a false positive reading.  The sigma estimate if more than 4 time smaller for subgroup size of 30 compared to a subgroup size of 5.


\section*{Question 2}

You are presented with data that are obviously autocorrelated (perhaps similar to Figure 4.6). What is the first thing that you should try to do?

\subsection*{answer}
When presented with data that is autocorrelated, we should graph the raw data.

\section*{Question 3}
You are presented with the data in Exercise \#6 on page 129, which you used in working Assignment One. Construct a normal probability plot of the subgroup averages. Does this explain why there are so many subgroup averages that fall outside the limits on the X-bar chart? If not, what is the explanation, or is there an explanation? What lesson is learned here, if any?

<<q3_work>>=
q3 <-read.csv("data/Assigment1Q1.csv")

@ 
<<q3_scratch_plot,echo=F,fig=T,eps=F>>=
q3plot=ggplot(q3) +stat_qq(aes_string(sample="xBar"))+theme_bw()+ylim(10,30) +
  geom_abline(intercept=19.85,colour="red",slope=5)
plot=q3plot+opts(title="x bar plot") + ylab("xBar Sample")
print(plot)
@ 

<<q3_poke>>=
summary(q3plot)
@ 

<<q3_tikz_plot,echo=F>>=
q3plottikz <- q3plot+opts(title="\\(\\bar{X}\\) qq plot") +ylab("\\(\\bar{X}\\) quantiles")
tikz(file="tik/session3_q3.tex",standAlone=TRUE)
print(q3plottikz)
dev.off()
system("/usr/texbin/pdflatex -output-directory tik tik/session3_q3.tex")
@ 

\subsection*{answer}

\includegraphics{tik/session3_q3.pdf}

As we can see from the qq plot above the normality assumption is violated for this dataset. The values below the mean are higher than predicted and the values above the mean are lower in quantile than predicted. We should check normality assumptions before attempting to establish a monitoring regime.

\section*{Question 4}

Assume that a company has some chemical data that are naturally autocorrelated, as explained on page 143 for the data in Table 5.3. What estimator should be used to estimate the standard deviation for such data?

\subsection*{answer}

 I am looking for an estimator that can be used without subgrouping.
 This consideration eliminates $\bar{R}$ and $\bar{X}$ estimators from
 sections 4.7.2 and 4.7.6 respectively.  Of the two choices for X - chart estimators covered in the course, I would choose
 {\large$\sfrac{s}{c_{4}} $} as a standard deviation estimator. 



\section*{Question 5}


Assume that a time sequence plot of individual observations suggests that the mean was not constant throughout the data. What estimator should be used to estimate the standard deviation and construct initial control limits? Explain why.


\subsection*{answer}
I would choose { \large$\sfrac{s}{c_{4}} $} as a standard deviation estimator.  This esimator uses all known data in the Stage I phase. 
\end{document}
